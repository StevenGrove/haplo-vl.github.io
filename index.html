<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Haplo: A Single-Transformer Baseline for Multi-Modal Understandinge</title>
  <link rel="icon" type="image/x-icon" href="static/images/bot.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Haplo: A Single-Transformer Baseline for Multi-Modal Understandinge</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="FIRST AUTHOR PERSONAL LINK" target="_blank" style="color:#50b08f !important">Rui Yang</a>,</span>
                <span class="author-block">
                  <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Lin Song</a><sup>‚úâÔ∏è</sup>,</span>
                  <span class="author-block"><a href="THIRD AUTHOR PERSONAL LINK" target="_blank" style="color:#b249f8 !important">Yicheng Xiao</a>,</span>
                  <span class="author-block"><a href="THIRD AUTHOR PERSONAL LINK" target="_blank" style="color:#50b08f !important">Runhui Huang</a>,</span>
                  <span class="author-block"><a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Yixiao Ge</a>,</span>
                  <span class="author-block"><a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Ying Shan</a>,</span>
                  <span class="author-block"><a href="THIRD AUTHOR PERSONAL LINK" target="_blank" style="color:#50b08f !important">Hengshuang Zhao</a><sup>‚úâÔ∏è</sup></span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><b style="color:#50b08f; font-weight:normal">&#x25B6 </b> The University of Hong Kong</b></span>
                    <span class="author-block"><b style="color:#008AD7; font-weight:normal">&#x25B6 </b> ARC Lab, Tencent</span>
                    <span class="author-block"><b style="color:#b249f8; font-weight:normal">&#x25B6 </b> Tsinghua University</span>
                  </div>

                  <!-- <div class="is-size-5 publication-authors">
                    <span class="author-block">The University of Hong Kong</span>
                    <span class="author-block">ARC Lab, Tencent</span>
                    <span class="eql-cntrb"><small><br><sup>‚úâÔ∏è</sup>Indicates Corresponding Authors</small></span>
                  </div> -->

                  <div class="column has-text-centered">
                    <div class="publication-links">
                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/YOUR REPO HERE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                <!-- Huggingface Link -->
                <span class="link-block">
                  <a href="https://huggingface.co/YOUR_MODEL" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    ü§ó
                  </span>
                  <span>Models</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <!-- Your video here -->
        <source src="static/videos/banner_video.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus. 
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Recent advancements in large language models (LLMs) have significantly propelled the development of large multi-modal models (LMMs), highlighting the potential for general and intelligent assistants. However, most LMMs model visual and textual modalities separately, leading to recent efforts to develop native LMMs using one transformer. Despite the promise, these native models are resource-intensive and often exhibit performance gaps compared to their compositional counterparts. To alleviate this issue, we propose a simple yet efficient method to construct a baseline for the native and end-to-end large multi-modal model in a single transformer. First, we propose a new multi-modal transformer model that can fuse multi-modal inputs in the early stage and respond to visual instructions in an auto-regressive manner. Second, we devise an efficient training recipe for the proposed model, which harnesses the prior knowledge of the pre-trained models, addressing both the performance limitations and the challenge of resource consumption. The proposed model demonstrates superior performance compared to other LMMs using one transformer and significantly narrows the performance gap with compositional LMMs.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Paper method -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <h2 class="title is-3 has-text-centered">Method</h2>

        <!-- Subtitle: Three Stage Training -->
        <h3 class="subtitle is-4" style="margin-top: 20px;">Model Architecture:</h3>


        <!-- Subtitle: Three Stage Training -->
        <h3 class="subtitle is-4" style="margin-top: 20px;">Three Stage Training:</h3>

        <!-- Image section -->
        <div class="has-text-centered" style="margin-top: 20px;">
          <img src="static/images/train-pipeline.png" alt="Method Illustration" style="max-width: 100%; height: auto;">
          <p class="is-italic" style="text-align: left; margin-top: 10px;">
            Figure 1: The training pipeline of our Haplo. ME-Former is the proposed multi-modal early fusion transformer that can input images and text at the same time. In the modal expansion stage (a), we train ME-Former by distilling from the pre-trained vision encoder and the text embedding of the LLM. In the alignment stage (b), we tune the multi-modal projector (MM-Projector) and freeze other components. The whole model is tuned in the instruction tuning stage (c).
          </p>
        </div>

        <!-- Additional text (optional) -->
        <!-- <div class="content has-text-justified" style="margin-top: 20px;">
          <p>
            The method involves several key steps, including data preprocessing, model training, and evaluation. 
            The above figure provides a visual overview of the process.
          </p> -->
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper method -->


<!-- Paper results -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <h2 class="title is-3 has-text-centered">Results</h2>

        <!-- Subtitle: Main results -->
        <h3 class="subtitle is-4" style="margin-top: 20px;">Main results</h3>

        <!-- Image section -->
        <div class="has-text-centered" style="margin-top: 20px;">
          <img src="static/images/results/main_results.png" alt="Main results" style="max-width: 100%; height: auto;">
          <p class="is-italic" style="text-align: left; margin-top: 10px;">
            Table 1: Comparison on multi-modal benchmarks. `*' denotes images of related training datasets are observed during training. Haplo-8B-MI is the model further fine-tuned on multi-image datasets.
          </p>
        </div>

        <!-- Subtitle: Ablation study -->
        <h3 class="subtitle is-4" style="margin-top: 20px;">Ablation study</h3>

        <!-- Subtitle: ablation 1 -->
        <h3 class="subtitle is-5" style="margin-top: 20px;">Ablation for different LLMs, resolution, and visual instruction data:</h3>

        <!-- Image section -->
        <div class="has-text-centered" style="margin-top: 20px;">
          <img src="static/images/results/ablation_study_1.png" alt="ablation study" style="max-width: 50%; height: auto;">
          <p class="is-italic" style="text-align: left; margin-top: 10px;">
            Table 2: Ablation for different LLMs, resolution (Res.), and visual instruction data (Data-S3). `*' denotes that images from training datasets are used during training.
          </p>
        </div>

        <div class="content">
          <ul>üåü A more advanced language model delivers significantly superior results.</ul>
          <ul>üåü Higher input resolution enhances performance, as the LMM can capture finer-grained visual details.</ul>
          <ul>üåü Expanding the visual instruction tuning data leads to substantial improvements by enriching the LMM's knowledge.</ul>
        </div>

        <!-- Subtitle: ablation 2 -->
        <h3 class="subtitle is-5" style="margin-top: 20px;">Ablation for modal expansion stage (stage 1):</h3>

        <div class="has-text-centered" style="margin-top: 20px;">
          <img src="static/images/results/ablation_study_2.png" alt="ablation study" style="max-width: 80%; height: auto;">
          <p class="is-italic" style="text-align: left; margin-top: 10px;">
            Table 3: Ablation for modal expansion stage.
          </p>
        </div>

        <div class="content">
          <ul>üåü Modal expansion stage accelerates the convergence process.</ul>
        </div>

        <!-- Subtitle: ablation 3 -->
        <h3 class="subtitle is-5" style="margin-top: 20px;">Compared with the compositional LMM using the same LLM and training data:</h3>

        <div class="columns">
          <div class="column">
            <img src="static/images/results/ablation_3_1.png" alt="Image 1" style="width: 100%; height: auto;">
            <p class="is-italic" style="text-align: left; margin-top: 10px;">
              Table 4: Comparison with LLaVA-1.5-7B using the same LLM, resolution and 665K instruction data. `One Trans.' indicates whether the model belongs to one multi-modal transformer. All models depend on Vicuna-7B and 665K instruction data.
            </p>
          </div>
          <div class="column">
            <img src="static/images/results/ablation_3_2.png" alt="Image 2" style="width: 100%; height: auto;">
            <p class="is-italic" style="text-align: left; margin-top: 10px;">
              Table 5: Comparison with LLaVA-1.5-7B on MMStar. CP: coarse perception, FP: fine-grained perception, IR: instance reasoning, LR: logical reasoning, ST: science and technology, and MA: mathematics.
            </p>
          </div>
        </div>

        <div class="content">
          <ul>üåü Early fusion of the textual and visual embeddings is beneficial for fine-grained perception.</ul>
        </div>

        <!-- Subtitle: Qualitative results -->
        <h3 class="subtitle is-4" style="margin-top: 20px;">Case study</h3>

        <!-- Additional text (optional) -->
        <!-- <div class="content has-text-justified" style="margin-top: 20px;">
          <p>
            The method involves several key steps, including data preprocessing, model training, and evaluation. 
            The above figure provides a visual overview of the process.
          </p>
        </div> -->
      </div>
    </div>
  </div>
</section>
<!-- End paper results -->


<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/carousel1.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          First image description.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/carousel2.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Second image description.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/carousel3.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
         Third image description.
       </h2>
     </div>
     <div class="item">
      <!-- Your image here -->
      <img src="static/images/carousel4.jpg" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        Fourth image description.
      </h2>
    </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
